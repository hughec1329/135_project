\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{hyperref}
\title{STA242 - HW05 - Parallel Computing}
\author{Hugh Crockford}
\date{\today}
\begin{document}
	\maketitle
	\section{Description of the Data}

		The project is looking at relationships between Facebook networks, and the data are a sampling of almost a million Facebook users and their friends.
		Another file contains the properties of each user, consisting of total number of friends and the networks they belonged to.


		
		\subsection{Preparatory data munging}
		We were interested in the network structure of the data, and hence needed to group the bulk friends list by network.


		We initially tried to use shell tools to split the friends file into multiple separate network files. % (getnet.sh,stripnet.sh).
		Awk was used to match net id to each person's network membership and return their line number, then copy that line into another file for each network. 
		We tried head/tail to extract the line, but this was very slow. After trying multiple approaches the fastest method used sed, but even this took 3 hours to do a network with 5000 members, and unnecessarily copied the file.


		To get the project done this year, we'd need to use databases.
		The number of friends of each user varied, so if each friend had its own column each row in the table would have differnt lengths and not be indexable.
		To overcome this limitation each line was split into an integer of user id and a string containing all friends, resulting in a two column table indexed on user id for fast retrieval.
		The friends file was space separated so a combination of sed and regular expressions was used to translate the first space into a comma to allow importing the file into monetdb as a csv file.


		After setting up a monet user for R with password stored in user file so we didn't need to enter it each query, the data could be returned from R using a db connection, but we still needed to identify which users were members a particular network.
		A system call was used to the awk/shell command above to return the line numbers of each member. %(justgetnum.sh) 
		Using paste and sprintf, the database query was created and results returned as a data frame which could be transformed to an edgelist and network variables computed using igraph package.


		The queries from R worked initially, but the repeated requests when query was within an apply caused the db file to grow exponentially.
		On loading the data the db was ~14 gb, but with each successive query, the db file size would increase substantially until the hdd was full and monet couldn't process query as it had nowhere to store results.
		Following some investigation we found users complaining of a similar issue on Stack Overflow, and a few answers from the monet dev team blaming the OS's handling of I/O, as monet doesn't do memory managing itself. 



		\subsection{Calculating variables}
		After this preparatory work an sapply was run over the most popular 1000 networks ( identified with a piped grep,cut,sort,uniq -c command).
		This took a very long time, and unfortunately initially the script didnt save any interim data, so after running for 24 hours we cancelled the command and had nothing to show for it.


		Slight alteration of the script allowed its output to be updated during each iteation.
		A system call to echo appending results to a file gave us running results and permitted the script's output to be monitored (with tail -f and wc -l) and after each iteration an email progress update was sent from a local smtp server.


		This script was started on 1000 topnets, but this time the list was inverted to get variables on the faster smaller nets first.

		\subsection{Parallelizing the process - Amazon EC2}
		While this script was running on a local machine we rented an Amazon EC2 extra large spot instance, with 30gb ram and 8 cores.
		Using a tmux session though ssh allowed simontaneuos control of R and output monitoring, and let the script continue through any disconnects or broken pipes.
		After downloading and setting up monetdb the data was wget from web as scp from a local machine was slow, and the database set up as explained above.


		Cloning the github repo was a fast way to transfer all script files to the cloud machine, and once the database was set up a cluster was created in R with with fork from package parallel, and set it running via clusterApplyLB on topnets.
		The script ran as expected, and the 8 processes spiking on top confirmed it was working in parallel, but no output was produced and it looked like R had crashed.
		After debugging it was discovered monetdb couldn't handle the multiple simultaneous requests and required a reboot each time the file was run. 


		To overcome this problem, the process was split into two steps, the fast data extraction from database in the first function was achieved in a regular sapply which allowed monet to keep up, and second slower variable calculation was parallelized in the second function.
		Using clusterApplyLB over the 8 cores signifigantly sped up the calculations, and we collected summary variables from ~900 networks in 24 hours.

	\section{Git}
		 A github repo containing all relevant files can be found at: https://github.com/hughec1329/

\newpage
	\section{CODE}
	\subsection{R}
		\lstinputlisting[breaklines=TRUE]{parStrippwer.R}
	\newpage
	\subsection{shell}
		\lstinputlisting[breaklines=TRUE]{getnet.sh}
		\lstinputlisting[breaklines=TRUE]{makeedge.sh}
		\lstinputlisting[breaklines=TRUE]{stripnet.sh}
\end{document}
